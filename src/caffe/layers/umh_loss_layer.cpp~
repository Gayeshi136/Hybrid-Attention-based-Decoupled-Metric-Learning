//unsupervised, RBF-like exp(-|xi-xi+|^2/2 )/[exp(-|xi-xi+|^2/2 )+sum(exp(-|xi-xj|^2/2 ))]   对比负样本时候不能用（xi+xi+）/2 不收涟
#include <algorithm>
#include <vector>
#include "stdio.h"
#include "caffe/filler.hpp"
#include "caffe/layer.hpp"
#include "caffe/layers/umh_loss_layer.hpp"
#include "caffe/util/io.hpp"
#include "caffe/util/math_functions.hpp"
#include <cmath>
namespace caffe {

template <typename Dtype>
void UMHLossLayer<Dtype>::LayerSetUp(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  LossLayer<Dtype>::LayerSetUp(bottom, top);
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  diff_temp_.Reshape(1,bottom[0]->channels(), 1, 1); //存储临时差量
  center_temp_.Reshape(1,bottom[0]->channels(), 1, 1); //存储临时差量
  diff_.Reshape(bottom[0]->num(),bottom[0]->channels(), 1, 1);
  center_temp1_.Reshape(1,bottom[0]->channels(),1,1);
}
template <typename Dtype>
void UMHLossLayer<Dtype>::Forward_cpu(
    const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
    Dtype threshold = this->layer_param_.umh_loss_param().threshold();
    const int negative_num = this->layer_param_.umh_loss_param().negative_num();
    Dtype ksi=this->layer_param_.umh_loss_param().ksi();
        
   int num = bottom[0]->num();//
    int channels = bottom[0]->channels();
    Dtype loss(0.0);
    Dtype k = 1.0;
	
	Dtype* diff = diff_.mutable_cpu_data();
	Dtype* diff_temp = diff_temp_.mutable_cpu_data();
	Dtype* center_temp = center_temp_.mutable_cpu_data();
	Dtype* center_temp1 = center_temp1_.mutable_cpu_data();
	
    const Dtype* data = bottom[0]->cpu_data();
	float distance_temp=0;//用于存储临时distance
	caffe_set(channels*num, Dtype(0), diff);
	top[0]->mutable_cpu_data()[0]=0;
	for(int i=num/3; i<num/3*2; i++)
	{
		std::vector<std::pair<float, long int> > distance;//用于存储distance和序号
		std::vector<std::pair<float, long int> > distance_n;//用于存储distance和序号
		distance.push_back(std::make_pair(std::sqrt(1), i-num/3));//positive
		caffe_cpu_axpby(channels, Dtype(1), data + (distance[0].second)*channels, Dtype(0), center_temp);//计算k个邻近样本的中心
			for (int j=i+num/3; j<i+num/3 + negative_num; j++)  //计算all负类到中心的距离
			{	
				distance_n.push_back(std::make_pair(std::sqrt(1), j));		 
			}
				Dtype x_norm = std::sqrt(caffe_cpu_dot(channels,data+i*channels,data+i*channels));//xi norm
				Dtype center_norm = std::sqrt(caffe_cpu_dot(channels,center_temp,center_temp));//center norm
				Dtype distance_temp1 = caffe_cpu_dot(channels, data + i * channels, center_temp) * (Dtype(2)-Dtype(k)) + (Dtype(1)-Dtype(k))*x_norm*center_norm;;//正样本
				//找最大值
				Dtype max1=distance_temp1;
				for(int j=0;j<distance_n.size();j++){
				        Dtype temp_max =caffe_cpu_dot(channels, data + distance_n[j].second * channels, center_temp);
				        max1 = max1>=temp_max ? max1: temp_max;
				}
				distance_temp1 = exp(distance_temp1 - max1);//正样本
				distance_temp=0;//负样本
				for(int j=0;j<distance_n.size();j++)
				{
					distance_n[j].first = exp(caffe_cpu_dot(channels, data + distance_n[j].second * channels, center_temp) - max1);
					distance_temp+=distance_n[j].first;
				}

				loss = -log(std::max(Dtype(distance_temp1/(distance_temp1 + distance_temp)),Dtype(1e-20)));
				
				if(loss>=0)
				{       
				        caffe_copy(channels, center_temp, center_temp1);
				        caffe_cpu_axpby(channels, (Dtype(1)-k)*center_norm/x_norm, data + i * channels, Dtype(2)-k, center_temp1);
					caffe_cpu_axpby(channels, (distance_temp1/(distance_temp1 + distance_temp) - Dtype(1)), center_temp1, Dtype(1), diff+i*channels);//计算当前样本的梯度
					//更新center梯度
					caffe_copy(channels, data + i * channels, center_temp1);
					caffe_cpu_axpby(channels, (Dtype(1)-k)*x_norm/center_norm, center_temp, Dtype(2)-k, center_temp1);
					caffe_cpu_axpby(channels, distance_temp1/(distance_temp + distance_temp1), center_temp1, Dtype(0), diff_temp);
					for (int j = 0; j < distance_n.size(); j++){
					
					         caffe_cpu_axpby(channels, distance_n[j].first/(distance_temp + distance_temp1), data + distance_n[j].second * channels, Dtype(1), diff_temp);
					
					}
					caffe_sub(channels, center_temp1, diff_temp, diff_temp);
					caffe_cpu_axpby(channels,Dtype(-1),diff_temp,Dtype(1),diff+distance[0].second*channels);//计算当前样本的梯度
						
					for (int j=0;j<distance_n.size();j++)//更新负类样本梯度
					{ 
						caffe_cpu_axpby(channels, distance_n[j].first/(distance_temp + distance_temp1), center_temp, Dtype(1), diff+distance_n[j].second*channels);//计算最近负类和knncenter的梯度
					}
				}

		top[0]->mutable_cpu_data()[0] += loss;
	}
	top[0]->mutable_cpu_data()[0] =top[0]->mutable_cpu_data()[0]*3/num;
	
}

     

template <typename Dtype>
void UMHLossLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
	const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) 
{
    //update gradients
      caffe_cpu_axpby(bottom[0]->count(), top[0]->cpu_diff()[0]*Dtype(3)/bottom[0]->num(), diff_.cpu_data(), Dtype(0.0), bottom[0]->mutable_cpu_diff());
			
}




#ifdef CPU_ONLY
STUB_GPU(UMHLossLayer);
#endif

INSTANTIATE_CLASS(UMHLossLayer);
REGISTER_LAYER_CLASS(UMHLoss);

}  // namespace caffe
