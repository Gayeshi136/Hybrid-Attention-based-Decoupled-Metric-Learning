//iccv17 BIER Init Loss 配合 inner_product layer 中的 bier_init 使用
/*
#include <algorithm>

#include "stdio.h"
#include "caffe/layers/BIER_init_loss.hpp"
#include "caffe/util/io.hpp"
#include "caffe/util/math_functions.hpp"
#include <cmath>
namespace caffe {

template <typename Dtype>
void BIERInitLossLayer<Dtype>::LayerSetUp(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  LossLayer<Dtype>::LayerSetUp(bottom, top);
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  for(int i = 0; i < bottom.size(); i++){
        for(int j = i+1; j < bottom.size(); j++)
        temp_.push_back(vector<Dtype>(bottom[i]->channels()*bottom[j]->channels(),Dtype(0.)));//存储临时差量
  }
}
template <typename Dtype>
void BIERInitLossLayer<Dtype>::Reshape(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  top[0]->Reshape(1,1,1,1);
}
template <typename Dtype>
void BIERInitLossLayer<Dtype>::Forward_cpu(
    const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
    int num = bottom[0]->num();//
    int M = bottom.size();//number of learners
    top[0]->mutable_cpu_data()[0]=Dtype(0.);
    for(int i = 0; i < M; i++)
    {
        caffe_set(bottom[i]->count(),Dtype(0.),bottom[i]->mutable_cpu_diff());
    }
    Dtype loss = 0.;
    for(int n = 0; n < num; n++)
    {
                int cnt=0;
                
                for(int i = 0; i < M; i++)
                {
                        for(int j = i + 1; j < M; j++)
                        {
                                caffe_cpu_gemm(CblasNoTrans, CblasNoTrans, bottom[i]->channels(), bottom[j]->channels(), 1, Dtype(1), bottom[i]->cpu_data()+n*bottom[i]->channels(), bottom[j]->cpu_data()+n*bottom[j]->channels(), Dtype(0), &temp_[cnt][0]);
                                loss+=caffe_cpu_dot(bottom[i]->channels()*bottom[j]->channels(), &temp_[cnt][0], &temp_[cnt][0])/Dtype(bottom[i]->channels()*bottom[j]->channels());
                                cnt++;
                        }
                }
    }
    top[0]->mutable_cpu_data()[0] = loss/num;
}

     

template <typename Dtype>
void BIERInitLossLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
	const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) 
{
    //update gradients
    vector<Dtype> sum_temp(bottom.size(),Dtype(0.));
    for(int n=0; n<bottom[0]->num(); n++)
    {
        //compute each learner's norm sum
        for(int i=0; i<bottom.size(); i++)
        {
                sum_temp[i] = top[0]->cpu_diff()[0] * Dtype(2.0) * caffe_cpu_dot(bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels())/bottom[0]->num();
                
        }
        for(int i=0; i<bottom.size(); i++)
        {
                for(int j=0;j<bottom.size();j++)
                {
                        if(j==i)continue;
                        caffe_cpu_axpby(bottom[i]->channels(), sum_temp[j]/Dtype(bottom[i]->channels()*bottom[j]->channels()), bottom[i]->cpu_data() + n*bottom[i]->channels(), Dtype(1.0), bottom[i]->mutable_cpu_diff() + n*bottom[i]->channels());
                }
        }
        
    }
    
}




#ifdef CPU_ONLY
STUB_GPU(BIERInitLossLayer);
#endif

INSTANTIATE_CLASS(BIERInitLossLayer);
REGISTER_LAYER_CLASS(BIERInitLoss);

}  // namespace caffe

*/
//N组共同叉积
#include <algorithm>

#include "stdio.h"
#include "caffe/layers/BIER_init_loss.hpp"
#include "caffe/util/io.hpp"
#include "caffe/util/math_functions.hpp"
#include <cmath>
namespace caffe {

template <typename Dtype>
void BIERInitLossLayer<Dtype>::LayerSetUp(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  LossLayer<Dtype>::LayerSetUp(bottom, top);
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  F_iter_size_ = 0;
  B_iter_size_ = 0;
  for(int i = 0; i < bottom.size(); i++){
        for(int j = i+1; j < bottom.size(); j++)
        temp_.push_back(vector<Dtype>(bottom[i]->channels()*bottom[j]->channels(),Dtype(0.)));//存储临时差量
  }
}
template <typename Dtype>
void BIERInitLossLayer<Dtype>::Reshape(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  top[0]->Reshape(1,1,1,1);
}
template <typename Dtype>
void BIERInitLossLayer<Dtype>::Forward_cpu(
    const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
    int num = bottom[0]->num();//
    int M = bottom.size();//number of learners
    top[0]->mutable_cpu_data()[0]=Dtype(0.);
    //static int iter_size = 0;
    if(!this->layer_param_.bier_init_loss_param().perturbation())
    {
        //正常
        for(int i = 0; i < M; i++)
        {
                caffe_set(bottom[i]->count(),Dtype(0.),bottom[i]->mutable_cpu_diff());
        }
        Dtype loss = 0.;
        for(int n = 0; n < num; n++)
        {
                Dtype loss_temp=1.0;
                
                for(int i = 0; i < M; i++)
                {
                        loss_temp = loss_temp * caffe_cpu_dot(bottom[i]->channels(), bottom[i]->cpu_data() + n * bottom[i]->channels(), bottom[i]->cpu_data() + n * bottom[i]->channels())/bottom[i]->channels();
                        
                }
                loss= loss + loss_temp;
        }
        top[0]->mutable_cpu_data()[0] = loss/num;
    
        }
    else {
        //配合itersize 第一次正常第二次不计算
        if(F_iter_size == 0)
        {
                for(int i = 0; i < M; i++)
        {
                caffe_set(bottom[i]->count(),Dtype(0.),bottom[i]->mutable_cpu_diff());
        }
        Dtype loss = 0.;
        for(int n = 0; n < num; n++)
        {
                Dtype loss_temp=1.0;
                
                for(int i = 0; i < M; i++)
                {
                        loss_temp = loss_temp * caffe_cpu_dot(bottom[i]->channels(), bottom[i]->cpu_data() + n * bottom[i]->channels(), bottom[i]->cpu_data() + n * bottom[i]->channels())/bottom[i]->channels();
                        
                }
                loss= loss + loss_temp;
        }
        top[0]->mutable_cpu_data()[0] = loss/num;
    
                F_iter_size = 1;
        }else{
                top[0]->mutable_cpu_data()[0] = Dtype(0);
                F_iter_size = 0;
        }
    }
}

     

template <typename Dtype>
void BIERInitLossLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
	const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) 
{
    //update gradients
    vector<Dtype> sum_temp(bottom.size(),Dtype(0.));
    //static int iter_size = 0;
    if(!this->layer_param_.bier_init_loss_param().perturbation()){
        for(int n=0; n<bottom[0]->num(); n++)
        {
        //compute each learner's norm sum
                for(int i=0; i<bottom.size(); i++)
                {
                        sum_temp[i] = caffe_cpu_dot(bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels())/bottom[i]->channels();
                
                }
                for(int i=0; i<bottom.size(); i++)
                {
                        Dtype temp = 1.0;
                        for(int j = 0; j < bottom.size(); j++)
                        {
                                if(i!=j)
                                        temp = temp * sum_temp[j];
                        }
                        temp = temp * top[0]->cpu_diff()[0] * Dtype(2)/bottom[0]->num()/bottom[i]->channels();
                        caffe_cpu_axpby(bottom[i]->channels(), temp, bottom[i]->cpu_data() + n * bottom[i]->channels(), Dtype(1), bottom[i]->mutable_cpu_diff() + n * bottom[i]->channels());
                
                }
        
        }
    }else
    {
        if(B_iter_size_ == 0)
        {
        for(int n=0; n<bottom[0]->num(); n++)
        {
        //compute each learner's norm sum
                for(int i=0; i<bottom.size(); i++)
                {
                        sum_temp[i] = caffe_cpu_dot(bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels(), bottom[i]->cpu_data()+n*bottom[i]->channels())/bottom[i]->channels();
                
                }
                for(int i=0; i<bottom.size(); i++)
                {
                        Dtype temp = 1.0;
                        for(int j = 0; j < bottom.size(); j++)
                        {
                                if(i!=j)
                                        temp = temp * sum_temp[j];
                        }
                        temp = temp * top[0]->cpu_diff()[0] * Dtype(2)/bottom[0]->num()/bottom[i]->channels();
                        caffe_cpu_axpby(bottom[i]->channels(), temp, bottom[i]->cpu_data() + n * bottom[i]->channels(), Dtype(1), bottom[i]->mutable_cpu_diff() + n * bottom[i]->channels());
                
                }
        
        }
        B_iter_size_ = 1;
        }else
        {
                for(int i = 0; i < bottom.size(); i++)
                {
                caffe_set(bottom[i]->count(),Dtype(0.),bottom[i]->mutable_cpu_diff());
                }
                B_iter_size_ = 0;
        }
    }
    
}




#ifdef CPU_ONLY
STUB_GPU(BIERInitLossLayer);
#endif

INSTANTIATE_CLASS(BIERInitLossLayer);
REGISTER_LAYER_CLASS(BIERInitLoss);

}  // namespace caffe

