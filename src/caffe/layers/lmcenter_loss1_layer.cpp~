//  等長度centerloss
#include <algorithm>
#include <vector>

#include "caffe/layer.hpp"
#include "caffe/filler.hpp"
#include "caffe/loss_layers.hpp"
#include "caffe/util/io.hpp"
#include "caffe/util/math_functions.hpp"

namespace caffe {

template <typename Dtype>
void LMCenterLoss1Layer<Dtype>::LayerSetUp(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  LossLayer<Dtype>::LayerSetUp(bottom, top);
  CHECK_EQ(bottom[0]->height(), 1);
  CHECK_EQ(bottom[0]->width(), 1);
  CHECK_EQ(bottom[1]->channels(), 1);
  CHECK_EQ(bottom[1]->height(), 1);
  CHECK_EQ(bottom[1]->width(), 1);
  diff_.Reshape(bottom[0]->num(), bottom[0]->channels(), 1, 1);
  x_norm_.Reshape(bottom[0]->num(),1,1,1);
  diff_center_.Reshape(bottom[0]->num(), bottom[0]->channels(), 1, 1);
  center_norm_.Reshape(bottom[0]->num(),1,1,1);
  memory_.Reshape(bottom[0]->channels(),bottom[0]->channels(),1,1);
  memory1_.Reshape(bottom[0]->num(),bottom[0]->channels(),1,1);
  m_.Reshape(this->layer_param_.lmcenter_loss1_param().num_output(),1,1,1);

  const int axis = bottom[0]->CanonicalAxisIndex(
      this->layer_param_.lmcenter_loss1_param().axis());
  int K_ = bottom[0]->count(axis);
  if (this->blobs_.size() > 0) {
    LOG(INFO) << "Skipping parameter initialization";
  } else {
    this->blobs_.resize(1);
    // Intialize the weight
    vector<int> center_shape(2);
    center_shape[0] = this->layer_param_.lmcenter_loss1_param().num_output();
    center_shape[1] = K_;
    this->blobs_[0].reset(new Blob<Dtype>(center_shape));
    // fill the weights
    shared_ptr<Filler<Dtype> > center_filler(GetFiller<Dtype>(
        this->layer_param_.lmcenter_loss1_param().center_filler()));
    center_filler->Fill(this->blobs_[0].get());

  }  // parameter initialization
  this->param_propagate_down_.resize(this->blobs_.size(), true);
  variation_sum_.ReshapeLike(*this->blobs_[0]);
 ///////////////////////
}

template <typename Dtype>
void LMCenterLoss1Layer<Dtype>::Forward_cpu(
    const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
  int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  Dtype loss(0.0);
  
  Dtype* diff_center = diff_center_.mutable_cpu_data();
  Dtype* diff = diff_.mutable_cpu_data();

  const Dtype* center = this->blobs_[0]->cpu_data();
  const Dtype* data = bottom[0]->cpu_data();
  const Dtype* label = bottom[1]->cpu_data();
  Dtype* x_norm = x_norm_.mutable_cpu_data();
  Dtype* center_norm = center_norm_.mutable_cpu_data();
  
  for (int i=0; i<num; i++) {
  // x_norm
      x_norm[i] = caffe_cpu_dot<Dtype>(channels, data + i * channels, data + i * channels);
      x_norm[i] = static_cast<Dtype>(std::sqrt(x_norm[i]));
  //center_norm
      center_norm[i] = caffe_cpu_dot<Dtype>(channels, center + static_cast<int>(label[i]) * channels, center + static_cast<int>(label[i]) * channels);
      center_norm[i] = static_cast<Dtype>(std::sqrt(center_norm[i]));
  //compute diff_center = xi - ci/|ci|*|xi|
      caffe_cpu_axpby<Dtype>(channels, x_norm[i]/center_norm[i], center + static_cast<int>(label[i]) * channels, Dtype(0), diff_center + i * channels);
      caffe_sub<Dtype>(channels, data + i * channels, diff_center + i * channels, diff_center + i * channels);//
  //compute and update loss
      loss += caffe_cpu_dot<Dtype>(channels, diff_center + i * channels, diff_center + i * channels);
      //爲後面更新類中信做工作
      caffe_sub<Dtype>(channels, data + i * channels, center + static_cast<int>(label[i]) * channels, diff + i * channels );
  }
  loss = loss / static_cast<Dtype>(bottom[0]->num()) / Dtype(2);
  top[0]->mutable_cpu_data()[0] = loss;
}

template <typename Dtype>
void LMCenterLoss1Layer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
    const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {

  int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  const Dtype* label = bottom[1]->cpu_data();
  const Dtype* center = this->blobs_[0]->cpu_data();
  const Dtype* x_norm = x_norm_.cpu_data();
  const Dtype* center_norm = center_norm_.cpu_data();
  const Dtype* diff_center = diff_center_.cpu_data();
  Dtype* memory = memory_.mutable_cpu_data();
// compute back  xi-center/|center|*|xi| - (xi-center/|center|*|xi|)*(center' * xi)/|center|/|xi|
  for(int i = 0; i < num; i++){
       caffe_cpu_gemm<Dtype>(CblasTrans,CblasNoTrans,channels,channels,1,Dtype(1)/(x_norm[i]*center_norm[i]), center + static_cast<int>(label[i]) * channels, bottom[0]->cpu_data() + i * channels, Dtype(0), memory);
       caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, 1, channels, channels, Dtype(1), diff_center_.cpu_data() + i * channels, memory, Dtype(0), memory1_.mutable_cpu_data() + i * channels);
       caffe_sub<Dtype>(channels, diff_center + i * channels, memory1_.mutable_cpu_data() + i * channels, bottom[0]->mutable_cpu_diff() + i * channels);
       caffe_scal<Dtype>(channels, top[0]->cpu_diff()[0]/num, bottom[0]->mutable_cpu_diff() + i * channels);
}

  //update center
  if (this->param_propagate_down_[0]) {
    Dtype* center_diff = this->blobs_[0]->mutable_cpu_diff();
    Dtype* variation_sum_data = variation_sum_.mutable_cpu_data();
    const Dtype* distance_data = diff_.cpu_data();
    Dtype* m = m_.mutable_cpu_data();
    caffe_set(m_.count(),Dtype(0),m);

    // \sum_{y_i==j}
    caffe_set(variation_sum_.count(), (Dtype)0., variation_sum_.mutable_cpu_data());
    for (int i=0; i < num; i++) {
      caffe_sub(channels, variation_sum_data + static_cast<int>(label[i])*channels, distance_data + i*channels, variation_sum_data + static_cast<int>(label[i])*channels);
      m[static_cast<int>(label[i])] += 1;
    }
   for (int i = 0; i < this->layer_param_.lmcenter_loss1_param().num_output(); i++) {
      if (m[i]>0)
      caffe_cpu_axpby(channels, Dtype(1)/(m[i] + (Dtype)1.), variation_sum_data + static_cast<int>(label[i])*channels, Dtype(0), center_diff + static_cast<int>(label[i]) * channels);
    }
  }
}

#ifdef CPU_ONLY
STUB_GPU(LMCenterLoss1Layer);
#endif

INSTANTIATE_CLASS(LMCenterLoss1Layer);
REGISTER_LAYER_CLASS(LMCenterLoss1);

}  // namespace caffe
